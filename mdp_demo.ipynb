{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `pymdptoolbox` demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from mdptoolbox import mdp\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem\n",
    "* You have a 20-sided die, and you get to roll repeatedly until the sum of your rolls either gets as close as possible to 21 or you bust.\n",
    "* Your score is the numerical value of the sum of your rolls; if you bust, you get zero.\n",
    "* What is the optimal strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![state diagram](d20_mdp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what we have to deal with:\n",
    "* State space is 23-dimensional (sum of rolls can be 0-21 inclusive, plus the terminal state)\n",
    "* Action space is 2-dimensional (roll/stay)\n",
    "* State transitions are stochastic; requires transition matrix $T(s^\\prime;s,a)$\n",
    "* $T$ is mildly sparse (some transitions like 9->5 or 0->21 are impossible)\n",
    "* Rewards depend on both state and action taken from that state, but are not stochastic (only ever get positive reward when choosing \"stay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use the [*value iteration*](https://pymdptoolbox.readthedocs.io/en/latest/api/mdp.html#mdptoolbox.mdp.ValueIteration) algorithm. Looking at the documentation, we can see that it requires as input a transition matrx, a reward matrix, and a discount factor (we will use $\\gamma = 1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first specify the transition \"matrix\". It's going to be a 3-dimensional tensor of shape $(|\\mathcal{A}|,|\\mathcal{S}|,|\\mathcal{S}|) = (2, 23, 23)$. Most entries are probably zero, so let's start with a zero matrix and fill in the blanks. I'm going reserve the very last state (the 23rd entry) for the terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transition_matrix(n_sides=20, max_score=21):\n",
    "    \"\"\"Constructs the transition matrix for the MDP\n",
    "    \n",
    "    Arguments:\n",
    "        n_sides: number of sides on the die being rolled\n",
    "        max_score: the maximum score of the game before going bust\n",
    "    Returns:\n",
    "        np.ndarray: array of shape (A,S,S), where A=2, and S=max_score+2\n",
    "            representing the transition matrix for the MDP\n",
    "    \"\"\"\n",
    "    A = 2\n",
    "    S = max_score + 2\n",
    "    T = np.zeros(shape=(A, S, S))\n",
    "    p = 1/n_sides\n",
    "\n",
    "    # All the \"roll\" action transitions\n",
    "    # First, the transition from state s to any non terminal state s' has probability\n",
    "    # 1/n_sides unless s' <= s or s' > s + n_sides\n",
    "    for s in range(0, S-1):\n",
    "        for sprime in range(s+1, S-1):\n",
    "            if sprime <= s + n_sides:\n",
    "                T[0,s,sprime] = p\n",
    "    # The rows of T[0] must all sum to one, so all the remaining probability goes to\n",
    "    # the terminal state\n",
    "    for s in range(0, S-1):\n",
    "        T[0,s,S-1] = 1 - T[0,s].sum()\n",
    "    # It is impossible to transition out of the terminal state; it is \"absorbing\"\n",
    "    T[0,S-1,S-1] = 1\n",
    "\n",
    "    # All the \"stay\" action transitions\n",
    "    # This one is simple - all \"stay\" transitions dump you in the terminal state,\n",
    "    # regardless of starting state\n",
    "    T[1,:,S-1] = 1\n",
    "    \n",
    "    T[T<0] = 0 # There may be some very small negative probabilities due to rounding\n",
    "               # errors - this fixes errythang\n",
    "    \n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a peek at a smaller version \n",
    "T = make_transition_matrix(n_sides=4, max_score=5)\n",
    "print(\"roll transitions:\")\n",
    "print(T[0])\n",
    "print(\"\\nstay transitions:\")\n",
    "print(T[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the reward matrix. This is going to be a tensor of shape $(|\\mathcal{S}|,|\\mathcal{A}|) = (23,2)$. This one is even simpler than the transition matrix because only \"stay\" actions generate nonzero rewards, which are equal to the index of the state itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reward_matrix(max_score=21):\n",
    "    \"\"\"Create the reward matrix for the MDP.\n",
    "    \n",
    "    Arguments:\n",
    "        max_score: the maximum score of the game before going bust\n",
    "    Returns:\n",
    "        np.ndarray: array of shape (S,A), where A=2, and S=max_score+2\n",
    "            representing the reward matrix for the MDP\n",
    "    \"\"\"\n",
    "    A = 2\n",
    "    S = max_score + 2\n",
    "    R = np.zeros(shape=(S, A))\n",
    "    \n",
    "    # Only need to create rewards for the \"stay\" action\n",
    "    # Rewards are equal to the state index, except for the terminal state, which\n",
    "    # always returns zero\n",
    "    for s in range(0, S-1):\n",
    "        R[s,1] = s\n",
    "    \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a peek at a smaller version\n",
    "R = make_reward_matrix(max_score=5)\n",
    "print(\"roll rewards:\")\n",
    "print(R[:,0])\n",
    "print(\"\\nstay rewards:\")\n",
    "print(R[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now that we have the transition and reward matrices, our MDP is completely defined, and we can use the `pymdptoolbox` to help us figure out the optimal policy/strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sides = 20\n",
    "max_score = 21\n",
    "\n",
    "T = make_transition_matrix(n_sides, max_score)\n",
    "R = make_reward_matrix(max_score)\n",
    "\n",
    "model = mdp.ValueIteration(\n",
    "    transitions=T,\n",
    "    reward=R,\n",
    "    discount=1,\n",
    "    epsilon=0.001,\n",
    "    max_iter=1000,\n",
    ")\n",
    "model.setVerbose()\n",
    "model.run()\n",
    "print(f\"Algorithm finished running in {model.time:.2e} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That ran pretty fast, didn't it? Unfortunately most realistic MDP problems have millions or billions of possible states (or more!), so this doesn't really scale very well. But it works for our small problem very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's analyze the results. The `ValueIteration` object gives us easy access to the optimal value function and policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.V, marker='o')\n",
    "x = np.linspace(0, max_score, 10)\n",
    "plt.plot(x, x, linestyle=\"--\", color='black')\n",
    "ticks = list(range(0, max_score+1, 5)) + [max_score+1]\n",
    "labels = [str(x) for x in ticks[:-1]] + [\"\\u2205\"]\n",
    "plt.xticks(ticks, labels)\n",
    "plt.xlim(-1, max_score+2)\n",
    "plt.xlabel(\"State sum of rolls $s$\")\n",
    "plt.ylabel(\"State value $V$\")\n",
    "plt.title(\"MDP optimal value function $V^*(s)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.policy, marker='o')\n",
    "ticks = list(range(0, max_score+1, 5)) + [max_score+1]\n",
    "labels = [str(x) for x in ticks[:-1]] + [\"\\u2205\"]\n",
    "plt.xticks(ticks, labels)\n",
    "plt.xlim(-1, max_score+2)\n",
    "ticks = [0, 1]\n",
    "labels = [\"roll\", \"stay\"]\n",
    "plt.yticks(ticks, labels)\n",
    "plt.ylim(-0.25, 1.25)\n",
    "plt.xlabel(\"State sum of rolls $s$\")\n",
    "plt.ylabel(\"Policy $\\pi$\")\n",
    "plt.title(\"MDP optimal policy $\\pi^*(s)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the optimal policy is to keep rolling until the sum gets to 10. This is why $V(s) = s$ for $s>=10$ (black dashed line); because that's the score you end up with when following this policy. For $s<10$, it's actually a bit higher than $s$ because you get an opportunity to roll again to get a higher score, and the sum is low enough that your chances of busting are relatively low. We can see the slope is positive for $s \\le 21 - 20 = 1$ because it's impossible to bust below that point, but the slope becomes negative between $1 \\le s \\le 10$ because you're more likely to bust the higher you get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the state distribution $\\rho_\\pi(s_0 \\rightarrow s,t)$, which tells us the probability to be in any one of the states $s$ after a time $t$ when starting from state $s_0$:\n",
    "\n",
    "$$\n",
    "\\rho_\\pi(s_0 \\rightarrow s,t) = \\sum_{s^\\prime} T(s;s^\\prime,\\pi(s^\\prime)) \\rho_\\pi(s_0 \\rightarrow s^\\prime, t-1) \\\\\n",
    "\\text{where }\\rho_\\pi(s_0 \\rightarrow s, 0) = \\delta_{s, s_0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_state_distribution(policy, T, t_max=10):\n",
    "    S = len(policy)\n",
    "    # Reduce transition matrix to T(s';s) since policy is fixed\n",
    "    T_ = np.zeros(shape=(S, S))\n",
    "    for s in range(S):\n",
    "        for sprime in range(S):\n",
    "            T_[s,sprime] = T[policy[s],s,sprime]\n",
    "    T = T_\n",
    "    \n",
    "    # Initialize rho\n",
    "    rho = np.zeros(shape=(S, S, t_max+1))\n",
    "    for s in range(0, S):\n",
    "        rho[s,s,0] = 1\n",
    "        \n",
    "    # Use the iterative update equation\n",
    "    for t in range(1, t_max+1):\n",
    "        rho[:,:,t] = np.einsum(\"ji,kj->ki\", T, rho[:,:,t-1])\n",
    "    \n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = calculate_state_distribution(model.policy, T, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore') # Ignore the divide by zero error from taking log(0)\n",
    "    plt.imshow(np.log10(rho[0].T), cmap='viridis')\n",
    "cbar = plt.colorbar(shrink=0.35, aspect=9)\n",
    "cbar.ax.set_title(r\"$\\log_{10}(\\rho)$\")\n",
    "ticks = list(range(0, max_score+1, 5)) + [max_score+1]\n",
    "labels = [str(x) for x in ticks[:-1]] + [\"\\u2205\"]\n",
    "plt.xticks(ticks, labels)\n",
    "plt.xlabel(\"State sum of rolls $s$\")\n",
    "plt.ylabel(\"Number of rolls/turns $t$\")\n",
    "plt.title(r\"Optimal state distribution $\\rho_{\\pi^*}(s_0\\rightarrow s;t)$\")\n",
    "plt.subplots_adjust(right=2, top=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
